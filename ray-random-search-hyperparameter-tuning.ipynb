{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model.ram import RecurrentAttentionModel\n",
    "\n",
    "from data.augmented_mnist import minibatcher\n",
    "from data.augmented_mnist import get_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000, 10) 1.0 0.0\n",
      "(10000, 28, 28, 1) (10000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train),(X_test, y_test) = get_mnist(True, True, False)\n",
    "print(X_train.shape, y_train.shape, np.max(X_train), np.min(X_train))\n",
    "print(X_test.shape, y_test.shape, np.max(X_test), np.min(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 28, 28, 1) (45000, 10) 1.0 0.0\n",
      "(15000, 28, 28, 1) (15000, 10) 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape, np.max(X_train), np.min(X_train))\n",
    "print(X_val.shape, y_val.shape, np.max(X_val), np.min(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train(hyperparameter, X_train, y_train, X_val, y_val):\n",
    "    ram = RecurrentAttentionModel(time_steps=7,\n",
    "                                  n_glimpses=1, \n",
    "                                  glimpse_size=8,\n",
    "                                  num_classes=10,\n",
    "                                  max_gradient_norm=5.0,\n",
    "                                  std=hyperparameter[\"std\"])\n",
    "    optimizer = tf.keras.optimizers.Adam(hyperparameter[\"learning_rate\"])\n",
    "    for e in range(10):\n",
    "        # trainings step\n",
    "        batcher = minibatcher(X_train, y_train, hyperparameter[\"batch_size\"], True)\n",
    "        for X, y in batcher:\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = ram(X)\n",
    "                hybrid_loss, _, _, _ = ram.hybrid_loss(logits, y)\n",
    "\n",
    "                gradients = tape.gradient(hybrid_loss, ram.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, ram.trainable_variables))\n",
    "\n",
    "        # testing step\n",
    "        batcher = minibatcher(X_val, y_val, hyperparameter[\"batch_size\"], True)\n",
    "        accuracys = []\n",
    "        for X, y in batcher:\n",
    "            logits = ram(X)\n",
    "            accuracy, _, _ = ram.predict(logits, y)\n",
    "            accuracy = accuracy.numpy()\n",
    "            accuracys.append(accuracy)\n",
    "    return np.mean(accuracys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hyperparameters():\n",
    "    # Randomly choose values for the hyperparameters.\n",
    "    return {\"learning_rate\": 10 ** -np.random.uniform(0, 8),\n",
    "            \"batch_size\": np.random.randint(20, 200),\n",
    "            \"std\": 10 ** -np.random.uniform(0, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-16 09:49:28,805\tWARNING worker.py:1406 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2019-04-16 09:49:28,807\tINFO node.py:423 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-04-16_09-49-28_19313/logs.\n",
      "2019-04-16 09:49:28,948\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:44450 to respond...\n",
      "2019-04-16 09:49:29,081\tINFO services.py:363 -- Waiting for redis server at 127.0.0.1:13578 to respond...\n",
      "2019-04-16 09:49:29,084\tINFO services.py:760 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2019-04-16 09:49:29,132\tWARNING services.py:1236 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\n",
      "2019-04-16 09:49:29,134\tWARNING services.py:1261 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n",
      "2019-04-16 09:49:29,135\tINFO services.py:1384 -- Starting the Plasma object store with 20.0 GB memory using /tmp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.891263: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.898338: E tensorflow/stream_executor/cuda/cuda_driver.cc:320] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.898406: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:166] retrieving CUDA diagnostic information for host: 7576cf7f41cf\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.898424: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:173] hostname: 7576cf7f41cf\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.898559: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 410.104.0\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.898592: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 410.104.0\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.898604: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:308] kernel version seems to match DSO: 410.104.0\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.899005: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.933475: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600145000 Hz\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.934385: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1840d80 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:33.934446: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "\u001b[2m\u001b[36m(pid=19369)\u001b[0m 2019-04-16 09:49:34.371245: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1364] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "Accuracy is 0.257273405790329\n",
      "Accuracy is 0.3869015574455261\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "hyperparameter_configurations = [generate_hyperparameters()]\n",
    "\n",
    "# Launch some experiments.\n",
    "remaining_ids = []\n",
    "for hyperparameters in hyperparameter_configurations:\n",
    "    remaining_ids.append(train.remote(hyperparameters, X_train, y_train, X_val, y_val))\n",
    "\n",
    "# Whenever a new experiment finishes, print the value and start a new\n",
    "# experiment.\n",
    "for i in range(10):\n",
    "    ready_ids, remaining_ids = ray.wait(remaining_ids, num_returns=1)\n",
    "    accuracy = ray.get(ready_ids[0])\n",
    "    print(\"Accuracy is {}\".format(accuracy))\n",
    "    # Start a new experiment.\n",
    "    new_hyperparameters = generate_hyperparameters()\n",
    "    remaining_ids.append(train.remote(new_hyperparameters, X_train, y_train, X_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "0",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "176px",
    "left": "909px",
    "right": "50px",
    "top": "121px",
    "width": "321px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
